{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email Crawler",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1D5KnEuULcn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dee4942-e43b-48d8-d640-f66c81cd99f3"
      },
      "source": [
        "import re\n",
        "import requests\n",
        "from urllib.parse import urlsplit\n",
        "from collections import deque\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import xlrd\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "cols_list = [\"URLS\"]\n",
        "links = pd.read_csv(\"links.csv\", usecols=cols_list) \n",
        "links = links[\"URLS\"]\n",
        "\n",
        "duplicate = deque()\n",
        "for i in range(len(links)):\n",
        "  link = links[i]\n",
        "  duplicate.append(link)\n",
        "#duplicate\n",
        "\n",
        "\n",
        "scraped = set()\n",
        "emails = set()\n",
        "final_emails = []\n",
        "for t in range(len(duplicate)):\n",
        "  original_url = duplicate[t]\n",
        "  unscraped = deque([original_url])\n",
        "  emails.clear()\n",
        "  count =0\n",
        "  while len(unscraped) :\n",
        "      \n",
        "      if(count>10):\n",
        "        break\n",
        "      \n",
        "      url = unscraped.popleft()  \n",
        "      # print(\"unscraped is\" )\n",
        "      # print(unscraped)\n",
        "      scraped.add(url)\n",
        "\n",
        "      parts = urlsplit(url)\n",
        "          \n",
        "      base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
        "      if '/' in parts.path:\n",
        "        path = url[:url.rfind('/')+1]\n",
        "      else:\n",
        "        path = url\n",
        "\n",
        "      print(\"Crawling URL %s\" % url)\n",
        "      try:\n",
        "          response = requests.get(url)\n",
        "      except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
        "          continue\n",
        "\n",
        "      soup = BeautifulSoup(response.text, 'lxml')\n",
        "      \n",
        "      for anchor in soup.find_all(\"a\"):\n",
        "        if anchor.text != \"Contact Us\" and anchor.text != \"About Us\":\n",
        "          continue\n",
        "\n",
        "        #print(anchor.text)\n",
        "\n",
        "        \n",
        "        \n",
        "        if \"href\" in anchor.attrs:\n",
        "          link = anchor.attrs[\"href\"]\n",
        "        else:\n",
        "          link = ''\n",
        "        \n",
        "        \n",
        "        if link.startswith('/'):\n",
        "              # print(link)\n",
        "              link = base_url + link\n",
        "          \n",
        "          \n",
        "        elif not link.startswith('http'):\n",
        "              \n",
        "              link = path + link\n",
        "\n",
        "        len1 = len(link)\n",
        "        # if(link[len1-1] == '#' ):\n",
        "        #   break\n",
        "          \n",
        "        if not link.endswith(\".gz\"):\n",
        "          if not link in unscraped and not link in scraped:\n",
        "              unscraped.append(link)\n",
        "\n",
        "        # print(\"links are\")\n",
        "        #print(link)\n",
        "        \n",
        "      \n",
        "      \n",
        "      new_emails_com = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.com\", response.text, re.I))\n",
        "      new_emails_org = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.org\", response.text, re.I))\n",
        "      emails.update(new_emails_org)\n",
        "      emails.update(new_emails_com)\n",
        "      count = count +1\n",
        "  \n",
        "  e = []\n",
        "  for x in emails:\n",
        "    e.append(x)\n",
        "  \n",
        "  final_emails.append(e)\n",
        "  \n",
        "   \n",
        "print(final_emails)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crawling URL http://www.batyr.com.au\n",
            "Crawling URL http://www.batyr.com.au#\n",
            "Crawling URL http://www.batyr.com.au##\n",
            "Crawling URL http://www.batyr.com.au###\n",
            "Crawling URL http://www.batyr.com.au####\n",
            "Crawling URL http://www.batyr.com.au#####\n",
            "Crawling URL http://www.batyr.com.au######\n",
            "Crawling URL http://www.batyr.com.au#######\n",
            "Crawling URL http://www.batyr.com.au########\n",
            "Crawling URL http://www.batyr.com.au#########\n",
            "Crawling URL http://www.batyr.com.au##########\n",
            "Crawling URL http://www.aidafghanistanforeducation.org\n",
            "Crawling URL http://www.aidafghanistanforeducation.org#\n",
            "Crawling URL http://www.aidafghanistanforeducation.org/contact-us/\n",
            "Crawling URL http://www.aidafghanistanforeducation.org##\n",
            "Crawling URL http://www.aidafghanistanforeducation.org/contact-us/#\n",
            "Crawling URL http://www.aidafghanistanforeducation.org###\n",
            "Crawling URL http://www.aidafghanistanforeducation.org####\n",
            "Crawling URL http://www.aidafghanistanforeducation.org#####\n",
            "Crawling URL http://www.aidafghanistanforeducation.org######\n",
            "Crawling URL http://www.aidafghanistanforeducation.org#######\n",
            "Crawling URL http://www.aidafghanistanforeducation.org########\n",
            "Crawling URL http://www.somalilandmentalhealth.com/\n",
            "Crawling URL http://www.somalilandmentalhealth.com/#\n",
            "Crawling URL http://www.tanzdevtrust.org/\n",
            "Crawling URL https://tanzdevtrust.org/contact-us/\n",
            "Crawling URL https://tanzdevtrust.org/contact-us/\n",
            "[['jenya@batyr.com'], ['aidafgedu@gmail.com'], [], ['j.chapman@tanzdevtrust.org'], ['j.chapman@tanzdevtrust.org']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUbcXo6sUh0L"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open(\"new_file.csv\",\"w+\") as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "    csvWriter.writerows(final_emails)"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}